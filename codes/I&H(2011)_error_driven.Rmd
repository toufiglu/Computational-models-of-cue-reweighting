---
title: "Error-driven learning model"
output: html_document
date: "2023-12-08"
---
# set up
```{r load libraries and necessary functions, include=FALSE}
rm(list = ls())
source("libraries.R")
source("new_functions.R") # these are functions specific to these simulations.
source("parameters.R")
```

# prepare pre-training data
```{r load Chodroff and Wilson (2018) as pretraining data and  message=FALSE, warning=FALSE}
# Yiming: the set up of the training-test data and long-term knowledge is shared between the two models. So please make them a separate thing and just call them when running the error-driven model and the other models. For instance, you can keep this part in a parent rmd; then have the ideal adaptor model and the error-driven learning model both as child rmd files that will be called and run subsequently.

training_trials <- expand.grid(
    VOT = c(-20, -10, 0, 20, 30, 40),
    f0 = seq(220, 300, by=10),
    Talker = "experiment"
    ) %>% 
  mutate(category = case_when(
      VOT %in% c(-20, -10, 0) ~ "b",
      TRUE ~ "p"))

test_trials <- data.frame(
  VOT = 10,
  f0 = c(230, 230, 290, 290),
  Talker = 'experiment',
  category = c('b', 'p', 'b', 'p')
) # is this the right way to approach it? 
 # Yes, because adding them to the training was necessary to obtain cue weights for these values.
# The rationale here is similar to the pretraining of the ideal adaptor.

experiment_pretraining <- rbind(training_trials, test_trials) %>% 
  mutate(f0_Mel = phonR::normMel(f0)) %>% 
  

Long_term_knowledge <- readRDS("d.chodroff_wilson.rds") %>%
  filter(poa == "/b/-/p/") %>%
  droplevels() %>% 
  filter(
    between(VOT, mean(VOT) - 3 * sd(VOT), mean(VOT) + 3 * sd(VOT)),
    between(f0_Mel, mean(f0_Mel) - 3 * sd(f0_Mel), mean(f0_Mel) + 3 * sd(f0_Mel))) %>% 
  group_by(Talker, category) %>% 
  mutate(n = n()) %>%
  group_by(Talker) %>%
  mutate(
    n_min = min(n),
    n_category = n_distinct(category)) %>%
  # select talkers with both categories
  filter(n_category == 2) %>%
  group_by(Talker, category) %>%
  sample_n(size = first(n_min)) %>%
  ungroup() %>%
  dplyr::select(Talker, VOT, f0, f0_Mel, category) %>% 
  bind_rows(experiment_pretraining) %>% 
  mutate(
    f0_centered = round(apply_ccure(f0_Mel, data = .), 0),
    VOT_centered = round(apply_ccure(VOT, data = .), 0)
  )

experiment_pretraining <- Long_term_knowledge %>% 
  filter(Talker == 'experiment')

# reference table for normalization to be used later.
f0_reference <- experiment_pretraining %>% 
  dplyr::select(f0, f0_Mel, f0_centered) %>% 
  distinct()

VOT_reference <- experiment_pretraining %>% 
  dplyr::select(VOT, VOT_centered) %>% 
  distinct()

experiment_pretraining <- experiment_pretraining %>% 
  mutate (
    Frequency = 0,
    Cues = paste(paste0("normalised.vot", experiment_pretraining$VOT_centered), 
                 paste0("normalised.f0", experiment_pretraining$f0_centered),
                 sep = "_"),
    Outcomes = category
  ) %>% 
  dplyr::select(Cues, Outcomes, Frequency)
  

Long_term_pretraining <- Long_term_knowledge %>% 
  filter(Talker != 'experiment') %>% 
  mutate(category = as.character(category),
         category = gsub("/", "", category),
         Outcomes = category,
         Cues = paste(paste0("normalised.vot", round(VOT_centered)), 
                 paste0("normalised.f0", round(f0_centered)),
                 sep = "_")) %>% 
  count(Cues, Outcomes) %>% 
  mutate(Frequency = n) %>% 
  dplyr::select(-n)

pretraining <- rbind(Long_term_pretraining, experiment_pretraining)

set.seed(1)
w.pre<-estimateWeights(pretraining)
```

# test items
```{r prepare test items}
# The raw values for test items are VOT10, f0230,290. Here, we make reference to
# VOT_reference and f0_reference tables.

testStim<-rbind(c("normalised.vot22","normalised.f0199"),
                c("normalised.vot22","normalised.f0269"))
numTestStim<-dim(testStim)[1]
plot.data<-data.frame(testStim,stringsAsFactors=F)
names(plot.data)=c("VOT", "f0")
act.p<-mat.or.vec(numTestStim,maxRep)
act.b<-mat.or.vec(numTestStim,maxRep)
```


# canonical
```{r get canonical block exposure data and results message=FALSE, warning=FALSE, paged.print=FALSE}
canonical <- tribble(
  ~VOT, ~f0,
  -20, 230,
  -10, 220,
  -10, 230,
  -10, 240,
  0, 230,
  20, 290,
  30, 280,
  30, 290,
  30, 300,
  40, 290
) %>% format_training_data()

# get results for different learning rate
r_0.01_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.01)
r_0.1_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.1)
r_0.9_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.9)

# for the canonical block, which is the first block, independent and sequential models are identical.
r_0.01_canonical_sequential <- r_0.01_canonical_independent
r_0.1_canonical_sequential <- r_0.1_canonical_independent
r_0.9_canonical_sequential <- r_0.9_canonical_independent

# calculate the averaged /p/ probability across the ten blocks.
vars <- ls()
canonical_vars <- grep("^r_(0.01|0.1|0.9)_canonical", vars, value = TRUE)
canonical_list <- mget(canonical_vars)
canonical_results <- lapply(canonical_list, function(x) calculate_luce_choice(x))
canonical_trajectory <- lapply(canonical_list, function(x) extract_trajectory(x))
d.canonical <- as.data.frame(do.call(cbind, canonical_results))
```


# neutral
```{r get neutral block exposure data and results message=FALSE, warning=FALSE}
neutral = tribble(
  ~VOT, ~f0,
  -20, 260,
  -10, 250,
  -10, 260,
  -10, 270,
    0, 260,
    20, 260,
    30, 250,
    30, 260,
    30, 270,
    40, 260
   ) %>% format_training_data()

r_0.01_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.01, w_pre=w.pre)
r_0.1_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.1, w_pre=w.pre)
r_0.9_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.9, w_pre=w.pre)

r_0.01_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.01, w_pre=r_0.01_canonical_sequential$w[10][[1]])
r_0.1_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.1, w_pre=r_0.1_canonical_sequential$w[10][[1]])
r_0.9_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.9, w_pre=r_0.9_canonical_sequential$w[10][[1]])

vars <- ls()
neutral_vars <- grep("^r_(0.01|0.1|0.9)_neutral", vars, value = TRUE)
neutral_list <- mget(neutral_vars)
neutral_results <- lapply(neutral_list, function(x) calculate_luce_choice(x))
d.neutral <- as.data.frame(do.call(cbind, neutral_results))
neutral_trajectory <- lapply(neutral_list, function(x) extract_trajectory(x))

```


# reversed
```{r get reversed block exposure data and results message=FALSE, warning=FALSE}
reversed = tribble(
  ~VOT, ~f0,
  -20, 290,
  -10, 280,
  -10, 290,
  -10, 300,
    0, 290,
    20, 230,
    30, 220,
    30, 230,
    30, 240,
    40, 230
   ) %>% format_training_data()

r_0.01_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.01, w_pre=w.pre)
r_0.1_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.1, w_pre=w.pre)
r_0.9_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.9, w_pre=w.pre)

r_0.01_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.01, w_pre=r_0.01_neutral_sequential$w[10][[1]])
r_0.1_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.1, w_pre=r_0.1_neutral_sequential$w[10][[1]])
r_0.9_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.9, w_pre=r_0.9_neutral_sequential$w[10][[1]])

vars <- ls()
reversed_vars <- grep("^r_(0.01|0.1|0.9)_reversed", vars, value = TRUE)
reversed_list <- mget(reversed_vars)
reversed_results <- lapply(reversed_list, function(x) calculate_luce_choice(x))
d.reversed <- as.data.frame(do.call(cbind, reversed_results))
reversed_trajectory <- lapply(reversed_list, function(x) extract_trajectory(x))

```

# examine model architecture
The VOT values were firstly set to be identical to the rest of the blocks, and category labels are assigned in similar ways. But then, all VOT values were reset to 10 ms (normalized, 22 ms). We are interested in whether changing VOT values will provide a more dynamic readjustment of f0-to-category mapping. In fact, the results below show that the answer is yes. 
```{r to test the hypothesis that the limited re-weighting is due to the mixture of Gaussian architecture rather than the mechanism of the error-driven learning model, we keep VOT uninformative in this additional exposure block}
examine = tribble(
  ~VOT, ~f0,
  -20, 230,
  -10, 220,
  -10, 230,
  -10, 240,
    0, 230,
    20, 290,
    30, 280,
    30, 290,
    30, 300,
    40, 290
   ) %>% format_training_data() %>% 
  mutate(Cues = str_replace(Cues, "normalised\\.vot-?\\d+", "normalised.vot22"))

r_0.01_examine_independent <- Run_Rescorla(exposure=examine, rate=0.01, w_pre=w.pre)
r_0.1_examine_independent <- Run_Rescorla(exposure=examine, rate=0.1, w_pre=w.pre)
r_0.9_examine_independent <- Run_Rescorla(exposure=examine, rate=0.9, w_pre=w.pre)


vars <- ls()
examine_vars <- grep("^r_(0.01|0.1|0.9)_examine", vars, value = TRUE)
examine_list <- mget(examine_vars)
examine_results <- lapply(examine_list, function(x) calculate_luce_choice(x))
d.examine <- as.data.frame(do.call(cbind, examine_results))
examine_trajectory <- lapply(examine_list, function(x) extract_trajectory(x))

d.examine <- cbind(plot.data, d.examine) %>% 
  pivot_longer(cols=-c(VOT, f0), names_to = "Model", values_to = "prob.p") %>% 
  separate(Model, into = c("results", "rate", "block", "model"), sep = "_") %>%
  dplyr::select(-results) %>% 
  mutate(VOT=10,
         f0=ifelse(f0=="normalised.f0199", "Low f0", "High f0"),
         rate=paste0("rate=", rate)) %>% 
  rename(Levels=f0)

f.examine <- d.examine %>% 
  ggplot(aes(x = block, y = prob.p, group = Levels, color = Levels)) + 
  facet_grid(~rate) +
  geom_point(size = 2) +
  scale_color_manual(values = c("Low f0" = "orange", "High f0" = "blue")) +
  coord_cartesian(ylim = c(0, 1.1)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  theme(legend.text = element_text (size = 20),
        legend.position = "top",
        legend.title = element_text(size = 20, face = "bold"),
        axis.text.y = element_text(size = 20),
        axis.text.x = element_text(angle=22.5, hjust=1, size = 20),
        axis.title = element_text(size = 20, face = "bold"),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(color = "grey", size = 0.5),
        panel.grid.minor = element_line(color = "lightgrey", size = 0.25),
        strip.text.x = element_text(size = 20, face = "bold"),
        strip.text.y = element_text(size = 20, face = "bold")) +
  xlab ("Condition") + ylab("Proportion of /p/ responses") +
  coord_cartesian(ylim = c(0,1))
f.examine
```

# data processing
```{r merge results of the three blocks and prepare for ploting}
d.combined <- cbind(plot.data, d.canonical, d.neutral, d.reversed) %>% 
  pivot_longer(cols=-c(VOT, f0), names_to = "Model", values_to = "prob.p") %>% 
  separate(Model, into = c("results", "rate", "block", "model"), sep = "_") %>%
  dplyr::select(-results) %>% 
  mutate(VOT=10,
         f0=ifelse(f0=="normalised.f0199", "Low f0", "High f0")) %>% 
  rename(Levels=f0)
```

# averaged results figure
```{r averaged results plot for the error-driven model; part of this plot is figure 3 of the cogsci paper}
Figure3 <- d.combined %>% 
  ggplot(aes(x = block, y = prob.p, group = Levels, color = Levels)) + 
  geom_point(size = 2) +
  scale_color_manual(values = c("Low f0" = "orange", "High f0" = "blue")) +
  coord_cartesian(ylim = c(0, 1.1)) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  facet_grid(rate ~ model) +
  theme(legend.text = element_text (size = 20),
        legend.position = "top",
        legend.title = element_text(size = 20, face = "bold"),
        axis.text.y = element_text(size = 20),
        axis.text.x = element_text(angle=22.5, hjust=1, size = 20),
        axis.title = element_text(size = 20, face = "bold"),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(color = "grey", size = 0.5),
        panel.grid.minor = element_line(color = "lightgrey", size = 0.25),
        strip.text.x = element_text(size = 20, face = "bold"),
        strip.text.y = element_text(size = 20, face = "bold")) +
  xlab ("Condition") + ylab("Proportion of /p/ responses") +
  coord_cartesian(ylim = c(0,1))


Figure3
```
# trajectory figure
```{r get trajectory of /p/ responses}
vars <- ls()
trajectories <- grep("^(canonical|neutral|reversed)_trajectory", vars, value = TRUE)
trajectory_list <- mget(trajectories)
d.trajectory <- lapply(trajectory_list, function(x) get_trajectory_list(x))
d.trajectory <- do.call(rbind, d.trajectory)
rownames(d.trajectory) <- NULL

trajectory <- d.trajectory %>% 
  separate(condition, into = c("data", "rate", "block", "model"), sep = "_", remove = TRUE) %>% 
  dplyr::select(-data) %>% 
  mutate(block = factor(block, levels = c("canonical", "neutral", "reversed")))

# this is to plot the initial state at time 0
pretraining <- tibble(
  Iteration = 0,
  lowf0 = w.pre["normalised.f0199", "p"] * 199 + w.pre["normalised.vot22", "p"] * 22,
  highf0 = w.pre["normalised.f0269", "p"] * 269 + w.pre["normalised.vot22", "p"] * 22
)

blocks <- unique(trajectory$block)
models <- unique(trajectory$model)
rates <- unique(trajectory$rate)

# make the pretraining result present in every facet of the figure, but only show it in the canonical block to avoid having two data points at the same time step on x-axis.
pretraining_expanded <- expand_grid(pretraining, block = blocks, model = models, rate=rates) %>%
  filter(block=="canonical")

# Bind the pretraining_expanded tibble to the trajectory table
trajectory_combined <- bind_rows(pretraining_expanded, trajectory)

# plot the trajectory figure
figure_trajectory <- trajectory_combined %>% 
  pivot_longer(cols = c(lowf0, highf0), names_to = "variable", values_to = "value") %>% 
  mutate(Iteration=case_when(
    block=="canonical" ~ Iteration,
    block=="neutral" ~ Iteration+10,
    TRUE ~ Iteration+20
  )) %>% 
  ggplot(aes(x=Iteration, y=value, color=variable)) +
  facet_grid(rate~model) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "The trajectory of /p/ responses in the error-driven learning model",
       x = "iteration",
       y = "Activation weight",
       color = "f0") +
  scale_color_manual(values=c("highf0"="blue", "lowf0"="orange"))


figure_trajectory
```

# Xin: log likelihood as a means for model comparison.
=======
```{r model comparison}
# calculate log likelihoods (in the current setup, both error-driven learning models and the ideal adaptor models have only one free parameter)
d.error <- d.combined %>%
  filter(rate == "rate=0.1" & model == "sequential") %>%
  mutate(Levels = gsub(" f0", "", Levels)) %>%
  dplyr::select(block, prob.p, Levels) %>%
  pivot_wider(names_from = Levels, values_from = prob.p) %>%
  mutate(diff = High-Low)

d.io <- r_rep %>%
  filter(type == "sequential" &  prior_nu == 256 & prior_kappa == 16) %>%
  arrange(normalised_f0) %>%
  mutate(Levels = ifelse(normalised_f0 == 199, "Low", "High")) %>%
  dplyr::select(block, prob.p, Levels) %>%
  pivot_wider(names_from = Levels, values_from = prob.p) %>%
  mutate(diff = High-Low)

d.human <- IH_figure5 %>%
  arrange(desc(Levels)) %>%
  mutate(Levels = gsub(" f0", "", Levels)) %>%
  dplyr::select(-f0) %>%
  pivot_wider(names_from = Levels, values_from = prob.p) %>%
  mutate(diff = High-Low)

modlm.error <- lm(d.human$diff ~ d.error$diff)
logLik(modlm.error)

modlm.io <- lm(d.human$diff ~ d.io$diff)
logLik(modlm.io)

AIC(modlm.error, modlm.io)
BIC(modlm.error, modlm.io)

```

