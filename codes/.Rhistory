Cues = paste(paste0("normalised.vot", experiment_pretraining$VOT_centered),
paste0("normalised.f0", experiment_pretraining$f0_centered),
sep = "_"),
Outcomes = category
) %>%
select(Cues, Outcomes, Frequency)
Long_term_pretraining <- Long_term_knowledge %>%
filter(Talker != 'experiment') %>%
mutate(category = as.character(category),
category = gsub("/", "", category),
Outcomes = category,
Cues = paste(paste0("normalised.vot", round(VOT_centered)),
paste0("normalised.f0", round(f0_centered)),
sep = "_")) %>%
count(Cues, Outcomes) %>%
mutate(Frequency = n) %>%
select(-n)
pretraining <- rbind(Long_term_pretraining, experiment_pretraining)
set.seed(1)
w.pre<-estimateWeights(pretraining)
# The raw values for test items are VOT10, f0230,290. Here, we make reference to
# VOT_reference and f0_reference tables.
testStim<-rbind(c("normalised.vot22","normalised.f0199"),
c("normalised.vot22","normalised.f0269"))
numTestStim<-dim(testStim)[1]
plot.data<-data.frame(testStim,stringsAsFactors=F)
names(plot.data)=c("VOT", "f0")
act.p<-mat.or.vec(numTestStim,maxRep)
act.b<-mat.or.vec(numTestStim,maxRep)
canonical <- tribble(
~VOT, ~f0,
-20, 230,
-10, 220,
-10, 230,
-10, 240,
0, 230,
20, 290,
30, 280,
30, 290,
30, 300,
40, 290
) %>% format_training_data()
# get results for different learning rate
r_0.01_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.01)
r_0.1_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.1)
r_0.9_canonical_independent <- Run_Rescorla(exposure=canonical, w_pre=w.pre, rate=0.9)
# for the canonical block, which is the first block, independent and sequential models are identical.
r_0.01_canonical_sequential <- r_0.01_canonical_independent
r_0.1_canonical_sequential <- r_0.1_canonical_independent
r_0.9_canonical_sequential <- r_0.9_canonical_independent
# calculate the averaged /p/ probability across the ten blocks.
vars <- ls()
canonical_vars <- grep("^r_(0.01|0.1|0.9)_canonical", vars, value = TRUE)
canonical_list <- mget(canonical_vars)
canonical_results <- lapply(canonical_list, function(x) calculate_luce_choice(x))
d.canonical <- as.data.frame(do.call(cbind, canonical_results))
neutral = tribble(
~VOT, ~f0,
-20, 260,
-10, 250,
-10, 260,
-10, 270,
0, 260,
20, 260,
30, 250,
30, 260,
30, 270,
40, 260
) %>% format_training_data()
r_0.01_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.01, w_pre=w.pre)
r_0.1_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.1, w_pre=w.pre)
r_0.9_neutral_independent <- Run_Rescorla(exposure=neutral, rate=0.9, w_pre=w.pre)
r_0.01_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.01, w_pre=r_0.01_canonical_sequential$w[10][[1]])
r_0.1_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.1, w_pre=r_0.1_canonical_sequential$w[10][[1]])
r_0.9_neutral_sequential <- Run_Rescorla(exposure=neutral, rate=0.9, w_pre=r_0.9_canonical_sequential$w[10][[1]])
vars <- ls()
neutral_vars <- grep("^r_(0.01|0.1|0.9)_neutral", vars, value = TRUE)
neutral_list <- mget(neutral_vars)
neutral_results <- lapply(neutral_list, function(x) calculate_luce_choice(x))
d.neutral <- as.data.frame(do.call(cbind, neutral_results))
reversed = tribble(
~VOT, ~f0,
-20, 290,
-10, 280,
-10, 290,
-10, 300,
0, 290,
20, 230,
30, 220,
30, 230,
30, 240,
40, 230
) %>% format_training_data()
r_0.01_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.01, w_pre=w.pre)
r_0.1_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.1, w_pre=w.pre)
r_0.9_reversed_independent <- Run_Rescorla(exposure=reversed, rate=0.9, w_pre=w.pre)
r_0.01_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.01, w_pre=r_0.01_neutral_sequential$w[10][[1]])
r_0.1_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.1, w_pre=r_0.1_neutral_sequential$w[10][[1]])
r_0.9_reversed_sequential <- Run_Rescorla(exposure=reversed, rate=0.9, w_pre=r_0.9_neutral_sequential$w[10][[1]])
vars <- ls()
reversed_vars <- grep("^r_(0.01|0.1|0.9)_reversed", vars, value = TRUE)
reversed_list <- mget(reversed_vars)
reversed_results <- lapply(reversed_list, function(x) calculate_luce_choice(x))
d.reversed <- as.data.frame(do.call(cbind, reversed_results))
d.combined <- cbind(plot.data, d.canonical, d.neutral, d.reversed) %>%
pivot_longer(cols=-c(VOT, f0), names_to = "Model", values_to = "prob.p") %>%
separate(Model, into = c("results", "rate", "block", "model"), sep = "_") %>%
select(-results) %>%
mutate(VOT=10,
f0=ifelse(f0=="normalised.f0199", "Low f0", "High f0"),
rate=paste0("rate=", rate)) %>%
rename(Levels=f0)
Figure3 <- d.combined %>%
ggplot(aes(x = block, y = prob.p, group = Levels, color = Levels)) +
geom_point(size = 2) +
scale_color_manual(values = c("Low f0" = "orange", "High f0" = "blue")) +
coord_cartesian(ylim = c(0.2, 0.8)) +
scale_y_continuous(breaks = c(0.2, 0.5, 0.8)) +
facet_grid(rate ~ model) +
theme(legend.text = element_text (size = 20),
legend.position = "top",
legend.title = element_text(size = 20, face = "bold"),
axis.text.y = element_text(size = 20),
axis.text.x = element_text(hjust=1, size = 20),
axis.title = element_text(size = 20, face = "bold"),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(color = "grey", size = 0.5),
panel.grid.minor = element_line(color = "lightgrey", size = 0.25),
strip.text.x = element_text(size = 20, face = "bold"),
strip.text.y = element_text(size = 20, face = "bold")) +
xlab ("Condition") + ylab("Proportion of /p/ responses")
Figure3
Figure3
make_results_plot_representations <- function(data) {
# for clarity we only present results for a small range of values.
# Specifically, changing prior confidence in covariance does not effect results too much.
# We used nu=256, which means that listeners still largely rely on their prior experiences (it will take more than 256 input tokens to completely update previous experiences.)
# The three kappa values selected are representative of the general patterns we observe in the data.
# But readers are encouraged to delete this line and print the full range of results.
data <- data %>% filter(prior_kappa %in% c(16,64,256) & prior_nu == 256)
p.results <-
basic_IH_result_plot(data) +
facet_grid(
prior_kappa ~ type,
labeller = label_bquote(
rows = {kappa[.(categories.IH[1])] == kappa[.(categories.IH[2])]} == .(as.character(prior_kappa)))) +
ggh4x::force_panelsizes(cols = result.panel.size, rows = result.panel.size)
}
Figure4 <- make_results_plot_representations(r_rep)
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
source("libraries.R")
source("functions.R")
source("parameters.R")
# format exposure data
get_exposure_data <- function(input_data, Condition, n.subject=1) {
input_data <- input_data %>%
left_join(f0_reference, by='f0') %>%
left_join(VOT_reference, by='VOT') %>%
mutate(Condition=Condition,
Item.Type=Condition,
Item.Category = case_when(
VOT %in% c(-20, -10, 0) ~ "/b/",
TRUE ~ "/p/")) %>%
dplyr::select(-VOT, -f0, -f0_Mel) %>%
rename(VOT = VOT_centered,
f0_Mel = f0_centered) %>%
crossing(Subject = factor(1:n.subject)) %>%
mutate(Subject = paste(Condition, Subject, sep = "."),
Phase = 'training',
x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" = .y))) %>%
mutate(ItemID=as.character(row_number()))
names(input_data$x) <- NULL
return(input_data)
}
# a function used to deal with sequential models
model_for_sequential <- function(representation.model) {
representation.model <- representation.model %>%
mutate_at(vars(starts_with("prior_")), ~as.numeric(as.character(.x))) %>%
filter(Item.Intended_category == "/p/") %>%
select(prior_kappa, prior_nu, posterior) %>%
unnest(posterior)
}
# the following two functions will be used for processing results of the ideal adaptor and the normalization change model.
rep_processing <- function(variable) {
data <- get(variable, envir = .GlobalEnv)
results <-
data %>%
bind_rows() %>%
mutate(prob.p = ifelse(Item.Intended_category =="/p/", response, 1-response),
normalised_f0 = ifelse(Item.Intended_category =="/p/", 269, 199),
normalised_f0 = factor(normalised_f0)) %>%
group_by(Condition, prior_kappa, prior_nu, Item.Intended_category, normalised_f0) %>% # normalised_f0 is essentially determined by Item.Intended_category. It is included here to make sure that the column remains, which will be used for plotting below.
summarise(mean_prob_p = mean(prob.p), .groups = 'drop') %>%
rename(prob.p=mean_prob_p)
return(results)
}
norm_processing <- function(variable) {
data <- get(variable, envir = .GlobalEnv)
results <-
data %>%
bind_rows() %>%
mutate(prob.p = ifelse(Item.Intended_category =="/p/", response, 1-response),
normalised_f0 = ifelse(Item.Intended_category =="/p/", 269, 199),
normalised_f0 = factor(normalised_f0)) %>%
group_by(Condition, prior_kappa.normalization, Item.Intended_category, normalised_f0) %>%
summarise(mean_prob_p = mean(prob.p), .groups = 'drop') %>%
rename(prob.p=mean_prob_p)
return(results)
}
# visualization function
basic_IH_result_plot <- function(data) {
data %>%
mutate(Levels = ifelse(normalised_f0 == 199, "Low f0", "High f0")) %>%
ggplot(aes(x = block, y = prob.p, group = Levels)) +
list(
geom_point(size = 2, aes(color = Levels)),
scale_color_manual(values = c("Low f0" = "orange", "High f0" = "blue")),
coord_cartesian(ylim = c(0.2, 0.8)),
scale_y_continuous(breaks = c(0.2, 0.5, 0.8)),
scale_x_discrete(labels= c("canonical", "neutral", "reversed")),
xlab("Exposure condition"),
ylab("Proportion of /p/ responses"),
theme(
legend.key.height= duo.panel.key,
legend.position = "top",
axis.text.x = element_text(hjust=1, size = 20),
axis.text.y = element_text(size = 20),
axis.title.x = element_text(size = 20, face = "bold"),
axis.title.y = element_text(size = 20, face = "bold"),
legend.text = element_text (size = 20),
legend.title = element_text(size = 20, face = "bold"),
plot.title = element_text(size = 20),
strip.text.x = element_text(size= 20, face = "bold"),
strip.text.y = element_text(size= 20, face = "bold"),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(color = "grey", size = 0.5),
panel.grid.minor = element_line(color = "lightgrey", size = 0.25))
)
}
training_trials <- expand.grid(
VOT = c(-20, -10, 0, 20, 30, 40),
f0 = seq(220, 300, by=10),
Talker = "experiment"
) %>%
mutate(category = case_when(
VOT %in% c(-20, -10, 0) ~ "b",
TRUE ~ "p"))
test_trials <- data.frame(
VOT = 10,
f0 = c(230, 230, 290, 290),
Talker = 'experiment',
category = c('b', 'p', 'b', 'p')
)
experiment_pretraining <- rbind(training_trials, test_trials) %>%
mutate(f0_Mel = phonR::normMel(f0))
Long_term_knowledge <- readRDS("d.chodroff_wilson.rds") %>%
filter(poa == "/b/-/p/") %>%
droplevels() %>%
filter(
between(VOT, mean(VOT) - 3 * sd(VOT), mean(VOT) + 3 * sd(VOT)),
between(f0_Mel, mean(f0_Mel) - 3 * sd(f0_Mel), mean(f0_Mel) + 3 * sd(f0_Mel))) %>%
group_by(Talker, category) %>%
mutate(n = n()) %>%
group_by(Talker) %>%
mutate(
n_min = min(n),
n_category = n_distinct(category)) %>%
# select talkers with both categories
filter(n_category == 2) %>%
group_by(Talker, category) %>%
sample_n(size = first(n_min)) %>%
ungroup() %>%
select(Talker, VOT, f0, f0_Mel, category) %>%
bind_rows(experiment_pretraining) %>%
mutate(
f0_centered = round(apply_ccure(f0_Mel, data = .), 0),
VOT_centered = round(apply_ccure(VOT, data = .), 0)
)
experiment_pretraining <- Long_term_knowledge %>%
filter(Talker == 'experiment')
# reference table for normalization to be used later.
f0_reference <- experiment_pretraining %>%
select(f0, f0_Mel, f0_centered) %>%
distinct()
VOT_reference <- experiment_pretraining %>%
select(VOT, VOT_centered) %>%
distinct()
Long_term_knowledge <- Long_term_knowledge %>%
filter(Talker != 'experiment')
prior_marginal_VOT_f0_stats <-
Long_term_knowledge %>%
group_by(Talker) %>%
summarise(across(c(VOT, f0_Mel), mean)) %>%
ungroup() %>%
summarise(
x_mean = list(c(VOT = mean(VOT), f0 = mean(f0_Mel))),
x_var_VOT = var(VOT),
x_var_f0 = var(f0_Mel),
x_cov = list(cov(cbind(VOT, f0_Mel))))
m.VOT_f0_MVG <-
make_MVG_from_data(
data = Long_term_knowledge,
category = "category",
cues = cues)
m.io.VOT_f0.IH <- #make ideal observer for the I&H(2011) experiment, a bivariate Gaussian distribution.
m.VOT_f0_MVG  %>%
make_stop_VOTf0_ideal_observer() %>%
arrange(category)
m.ia.VOT_f0.IH <-
crossing(
prior_kappa = prior_kappa.plot,
prior_nu = prior_nu.plot) %>%
rowwise() %>%
mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.IH, kappa = .x, nu = .y))) %>%
unnest(ideal_adaptor) %>%
arrange(category)
canonical <- tribble(
~VOT, ~f0,
-20, 230,
-10, 220,
-10, 230,
-10, 240,
0, 230,
20, 290,
30, 280,
30, 290,
30, 300,
40, 290
)
neutral = tribble(
~VOT, ~f0,
-20, 260,
-10, 250,
-10, 260,
-10, 270,
0, 260,
20, 260,
30, 250,
30, 260,
30, 270,
40, 260
)
reversed = tribble(
~VOT, ~f0,
-20, 290,
-10, 280,
-10, 290,
-10, 300,
0, 290,
20, 230,
30, 220,
30, 230,
30, 240,
40, 230
)
d.IH.canonical <- get_exposure_data(canonical, Condition='canonical')
d.IH.neutral <- get_exposure_data(neutral, Condition='neutral')
d.IH.reversed <- get_exposure_data(reversed, Condition='reversed')
d.IH.test <- tibble(
VOT = 10,
f0 = c(230,290)
) %>%
left_join(f0_reference, by='f0') %>%
left_join(VOT_reference, by='VOT') %>%
select(f0_centered, VOT_centered) %>%
rename(f0_Mel=f0_centered, VOT=VOT_centered) %>%
mutate(Phase='test',
Item.Category=NA,
Item.Intended_category = case_when(
f0_Mel==199 ~ '/b/',
TRUE ~ '/p/'),
x=map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" = .y)),
Item.Type='test',
Block=1) %>%
crossing(Condition='test') %>%
mutate(Subject=paste(Condition, n.subject, sep = ".")) %>%
mutate(ItemID=as.character(row_number()))
update_representations_and_categorize_test <- function(
prior,
exposure,
test,
cues = c("VOT", "f0_Mel")
) {
update_NIW_ideal_adaptor_incrementally(
prior = prior,
exposure = exposure,
exposure.category = "Item.Category",
exposure.cues = cues,
noise_treatment = "marginalize",
lapse_treatment = "marginalize",
method = "label-certain",
keep.update_history = FALSE,
keep.exposure_data = FALSE) %>%
nest(posterior = everything()) %>%
add_test_and_categorize(test)
}
get_representation_models_for_plot <- function(
prior_kappa = prior_kappa.plot,
prior_nu = prior_nu.plot,
d.IH.exposure,
ideal_adaptor,
type
){
all_representations <- list()
Condition = d.IH.exposure$Condition[[1]]
if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.IH.results.representations_", example_label, "_", Condition, "_", type, ".rds")))){
for(i in 1:10){
set.seed(i)
# In IH11, exposure trials were presented in random order for each iteration of the 10 iterations. test trials are interspersed with exposure trials. We are unable to implement the interleaving exposure and test trials, but exposure trials are reshuffled in every run. After each iteration, the test trial results are obtained, and the /p/ responses for the two test trials are averaged over the 10 iterations.
d.IH.exposure<-d.IH.exposure[sample(nrow(d.IH.exposure)),]
if(i == 1){
# Use the ideal adaptor for the first iteration
m.ia.VOT_f0.IH.plot_sample <- ideal_adaptor
} else {
# Use the posterior from the previous iteration for the subsequent iteration
m.ia.VOT_f0.IH.plot_sample <- all_representations[[i-1]] %>%
mutate_at(vars(starts_with("prior_")), ~as.numeric(as.character(.x))) %>%
# at this point, there are always two bivariate Gaussian distributions for /b/ and /p/, respectively. Each of them will assign a posterior probability for the two test tokens, one is intended to be /b/ and another /p/. We end up having four rows for each kappa and nu combination. However, what we really need is just the two posterior Gaussian distributions. Therefore, the filter is used here.
filter(Item.Intended_category == "/p/") %>%
select(prior_kappa, prior_nu, posterior) %>%
unnest(posterior)
}
representations.pred <-
d.IH.exposure %>%
nest(data = -c(Condition, Subject)) %>%
crossing(
m.ia.VOT_f0.IH.plot_sample %>%
nest(prior = -c(prior_kappa, prior_nu))) %>%
group_by(Condition, Subject, prior_kappa, prior_nu) %>%
group_modify(~ update_representations_and_categorize_test(prior = .x$prior[[1]], exposure = .x$data[[1]], test = d.IH.test)) %>%
mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
mutate_at(vars(starts_with("prior_")), fct_rev) %>%
ungroup() %>%
mutate(Iteration = i)
# Ensure to add the result to the all_representations list
all_representations[[i]] <- representations.pred
saveRDS(all_representations, get_path(paste0("../models/d.IH.results.representations_", example_label, "_", Condition, "_", type, ".rds")))
}
}
else {
all_representations <- readRDS(get_path(paste0("../models/d.IH.results.representations_", example_label, "_", Condition, "_", type, ".rds")))
}
return(all_representations)
}
#canonical
representation_IH_canonical_independent <-
get_representation_models_for_plot(d.IH.exposure = d.IH.canonical, ideal_adaptor=m.ia.VOT_f0.IH, type="independent") %>%
lapply(function(x) {
mutate(x, Condition = "canonical_independent")
})
# as the canonical block is the first block, its independent and sequential results are the same.
representation_IH_canonical_sequential <- representation_IH_canonical_independent %>%
lapply(function(x) {
mutate(x, Condition = "canonical_sequential")
})
#neutral
representation_IH_neutral_independent <-
get_representation_models_for_plot(d.IH.exposure = d.IH.neutral, ideal_adaptor=m.ia.VOT_f0.IH, type="independent") %>%
lapply(function(x) {
mutate(x, Condition = "neutral_independent")
})
representation_IH_neutral_sequential <- get_representation_models_for_plot(d.IH.exposure = d.IH.neutral, ideal_adaptor=model_for_sequential(representation_IH_canonical_independent[[10]]), type="sequential") %>%
lapply(function(x) {
mutate(x, Condition = "neutral_sequential")
})
#reversed
representation_IH_reversed_independent <- get_representation_models_for_plot(d.IH.exposure = d.IH.reversed, ideal_adaptor=m.ia.VOT_f0.IH, type="independent") %>%
lapply(function(x) {
mutate(x, Condition = "reversed_independent")
})
representation_IH_reversed_sequential <- get_representation_models_for_plot(d.IH.exposure = d.IH.reversed, ideal_adaptor=model_for_sequential(representation_IH_neutral_sequential[[10]]), type="sequential") %>%
lapply(function(x) {
mutate(x, Condition = "reversed_sequential")
})
vars <- ls()
rep_models <- grep("^representation_IH_(canonical|neutral|reversed)", vars, value = TRUE)
r_rep <- map(rep_models, rep_processing) %>% bind_rows() %>%
separate(Condition,into = c("block", "type"), sep = "_", remove = FALSE)
make_results_plot_representations <- function(data) {
# for clarity we only present results for a small range of values.
# Specifically, changing prior confidence in covariance does not effect results too much.
# We used nu=256, which means that listeners still largely rely on their prior experiences (it will take more than 256 input tokens to completely update previous experiences.)
# The three kappa values selected are representative of the general patterns we observe in the data.
# But readers are encouraged to delete this line and print the full range of results.
data <- data %>% filter(prior_kappa %in% c(16,64,256) & prior_nu == 256)
p.results <-
basic_IH_result_plot(data) +
facet_grid(
prior_kappa ~ type,
labeller = label_bquote(
rows = {kappa[.(categories.IH[1])] == kappa[.(categories.IH[2])]} == .(as.character(prior_kappa)))) +
ggh4x::force_panelsizes(cols = result.panel.size, rows = result.panel.size)
}
Figure4 <- make_results_plot_representations(r_rep)
Figure4
Figure4
rm(list = ls())
library(ndl) # Yiming: add annotations for packages that are not commonly used and briefly describe why they are needed
library(tidyverse)
library(boot)
library(mgcv)
library(magrittr)
source("functions.R")
maxRep = 1000
# calculate log likelihoods (in the current setup, both error-driven learning models and the ideal adaptor models have only one free parameter)
d.error <- d.combined %>%
filter(rate == "rate=0.1" & model == "sequential") %>%
mutate(Levels = gsub(" f0", "", Levels)) %>%
select(block, prob.p, Levels) %>%
pivot_wider(names_from = Levels, values_from = prob.p) %>%
mutate(diff = High-Low)
